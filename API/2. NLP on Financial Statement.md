## Practice: Natural Language Processing Techniques in Python on Financial Statement

This practice sheet uses financial PDF document as the input corpus and focuses on **cleaning, tokenization, and lemmatization** (plus a few finance-aware preprocessing tricks).

> **How to use**
1. Put the PDF (you can change to your own file) in the same folder as the code. Here, our file's name is `financial_document.pdf`.
2. Set `PDF_PATH` below to your file name (or full path).
3. Run the cells in order.

---

## 0) Environment setup

```bash
!pip install pandas numpy matplotlib spacy pymupdf pdfplumber scikit-learn
!python -m spacy download en_core_web_sm
```

---

## 1) Load text path

```python
from pathlib import Path
PDF_PATH = "financial_document.pdf"  # <-- change to your PDF file name

```

### 1.1 Extract text (PyMuPDF)

```python
import fitz  # PyMuPDF

def extract_text_pymupdf(pdf_path: Path) -> str:
    text_parts = []
    with fitz.open(pdf_path) as doc:
        for page in doc:
            text_parts.append(page.get_text("text"))
    return "\n".join(text_parts)

raw_text = extract_text_pymupdf(PDF_PATH)
print(raw_text[:800])
print("\n---\nTotal characters:", len(raw_text))
```

### 1.2 (Optional) Fallback extraction (pdfplumber)

Use this if `raw_text` looks empty or messy.

```python
import pdfplumber

def extract_text_pdfplumber(pdf_path: Path) -> str:
    text_parts = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            txt = page.extract_text() or ""
            text_parts.append(txt)
    return "\n".join(text_parts)

# Uncomment if you want to compare:
# raw_text2 = extract_text_pdfplumber(PDF_PATH)
# print(raw_text2[:800])
```

---

## 2) Basic cleaning (whitespace, hyphenation, page breaks)

Financial PDFs often contain:
- line breaks inside sentences
- hyphenated words split across lines (e.g., `inter-\nnational`)
- repeated headers/footers (we handle that in section 3)

```python
import re

def basic_pdf_cleanup(text: str) -> str:
    # Remove common hyphenation across line breaks: "inter-\nnational" -> "international"
    text = re.sub(r"(\w)-\n(\w)", r"\1\2", text)

    # Replace newlines inside paragraphs with spaces (keep double-newline as paragraph break)
    text = re.sub(r"(?<!\n)\n(?!\n)", " ", text)

    # Normalize paragraph breaks
    text = re.sub(r"\n{2,}", "\n\n", text)

    # Collapse multiple spaces
    text = re.sub(r"[ \t]{2,}", " ", text)

    return text.strip()

clean_text = basic_pdf_cleanup(raw_text)
print(clean_text[:800])
```

---

## 3) (Optional) Remove repeated headers/footers (Not for the example file)

Many PDFs repeat the same header/footer lines on most pages.  
Below is a **lightweight heuristic**: detect short lines that occur very frequently and remove them.

```python
from collections import Counter

def remove_repeated_lines(text: str, min_occurrence_ratio: float = 0.02, max_line_len: int = 80) -> str:
    # Split back into lines to identify repeated short lines
    lines = [ln.strip() for ln in text.split("\n") if ln.strip()]
    n = len(lines)
    if n == 0:
        return text

    # Candidate: short lines only
    short_lines = [ln for ln in lines if len(ln) <= max_line_len]
    freq = Counter(short_lines)

    # Lines to remove: appear often enough
    threshold = max(2, int(min_occurrence_ratio * n))
    to_remove = {ln for ln, c in freq.items() if c >= threshold}

    # Rebuild while keeping paragraph boundaries from original text
    rebuilt = []
    for ln in text.split("\n"):
        if ln.strip() in to_remove:
            continue
        rebuilt.append(ln)
    return "\n".join(rebuilt)

clean_text2 = remove_repeated_lines(clean_text)
print(clean_text2[:800])
```

**Practice questions**
1. Print the top 10 most frequent short lines and see if they look like headers/footers.
2. Adjust `min_occurrence_ratio` or `max_line_len` and compare results.

---

## 4) Sentence segmentation and tokenization (spaCy)

We will use spaCy to:
- split text into sentences
- tokenize into words
- compute **lemmas** (lemmatization)

```python
import spacy
#Loads the small English spaCy language model(en_core_web_sm) and initializes the NLP processing pipeline.
nlp = spacy.load("en_core_web_sm", disable=["ner"])
#Adjusts the maximum allowed document length for spaCy processing.
nlp.max_length = max(nlp.max_length, len(clean_text) + 1000)

doc = nlp(clean_text)

#Extracts individual sentences from the processed document.
sentences = [s.text.strip() for s in doc.sents if s.text.strip()]
print("Number of sentences:", len(sentences))
print("\nExample sentences:\n- " + "\n- ".join(sentences[:5]))
```

---

## 5) Lemmatization (core)

### 5.1 Finance-aware stopwords (keep negation & uncertainty)

In financial text, words like **not**, **no**, **may**, **might**, **could** often carry real information.  
We remove standard stopwords but keep those.

```python
from spacy.lang.en.stop_words import STOP_WORDS

custom_stopwords = set(STOP_WORDS)
for w in ["not", "no", "nor", "may", "might", "could", "should"]:
    custom_stopwords.discard(w)

def lemmatize_tokens(doc):
    tokens = []
    for t in doc:
        if t.is_alpha:
            w = t.text.lower()
            if w in custom_stopwords:
                continue
            tokens.append(t.lemma_.lower())
    return tokens

lemmas = lemmatize_tokens(doc)
print("Number of lemma tokens:", len(lemmas))
print("Example lemmas:", lemmas[:30])
```

### 5.2 Quick check: lemma effect

```python
# Show some original -> lemma pairs (first 30 alphabetic tokens)
pairs = []
for t in doc:
    if t.is_alpha:
        pairs.append((t.text, t.lemma_))
    if len(pairs) >= 30:
        break

pairs
```


## 6) Finance-specific normalization (numbers and units)

In finance, we often want to preserve the *type* of numbers rather than the exact value, e.g.:
- `3.2%` → `<PCT>`
- `$2.5bn` → `<MONEY>`
- `50 bps` → `<BPS>`
- years → `<YEAR>`

```python
def finance_normalize_numbers(text: str) -> str:
    text = re.sub(r"\b\d+(\.\d+)?\s*%\b", " <PCT> ", text)
    text = re.sub(r"\$\s?\d+(\.\d+)?\s*(bn|b|m|k)?\b", " <MONEY> ", text, flags=re.I)
    text = re.sub(r"\b\d+(\.\d+)?\s*(bps|bp)\b", " <BPS> ", text, flags=re.I)
    text = re.sub(r"\b20\d{2}\b", " <YEAR> ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

normalized_text = finance_normalize_numbers(clean_text)
doc_norm = nlp(normalized_text)
lemmas_norm = lemmatize_tokens(doc_norm)

print("Example snippet (before):", clean_text[:200])
print("Example snippet (after): ", normalized_text[:200])
```

## 7) Word frequency: top terms after lemmatization

```python
from collections import Counter
import pandas as pd

freq = Counter(lemmas_norm)
top = freq.most_common(30)

df_top = pd.DataFrame(top, columns=["term", "count"])
df_top
```
